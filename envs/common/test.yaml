training:
  epochs: 100
  batch_size: 32
optimizer:
  name: Adam
  lr: 0.001
  weight_decay: 0.0001
model:
  layers: [64, 128, 64]
  dropout: 0.2